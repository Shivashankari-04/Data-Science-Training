{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Prepare Data in PySpark"
      ],
      "metadata": {
        "id": "HNIe0VXHFjV6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUpB4SZRE3ea",
        "outputId": "e909382f-d0d5-4fe1-85f0-b04199516376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID|Name |Department |Project        |Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|101  |Ravi |Engineering|AI Engine      |95000 |42          |\n",
            "|102  |Sneha|Engineering|Data Platform  |87000 |45          |\n",
            "|103  |Kabir|Marketing  |Product Launch |65000 |40          |\n",
            "|104  |Anita|Sales      |Client Outreach|70000 |38          |\n",
            "|105  |Divya|Engineering|AI Engine      |99000 |48          |\n",
            "|106  |Amit |Marketing  |Social Media   |62000 |35          |\n",
            "|107  |Priya|HR         |Policy Revamp  |58000 |37          |\n",
            "|108  |Manav|Sales      |Lead Gen       |73000 |41          |\n",
            "|109  |Neha |Engineering|Security Suite |91000 |46          |\n",
            "|110  |Farah|HR         |Onboarding     |60000 |36          |\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession, Row\n",
        "\n",
        "spark = SparkSession.builder.appName(\"EmployeeViews\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    Row(EmpID=101, Name=\"Ravi\", Department=\"Engineering\", Project=\"AI Engine\", Salary=95000, HoursPerWeek=42),\n",
        "    Row(EmpID=102, Name=\"Sneha\", Department=\"Engineering\", Project=\"Data Platform\", Salary=87000, HoursPerWeek=45),\n",
        "    Row(EmpID=103, Name=\"Kabir\", Department=\"Marketing\", Project=\"Product Launch\", Salary=65000, HoursPerWeek=40),\n",
        "    Row(EmpID=104, Name=\"Anita\", Department=\"Sales\", Project=\"Client Outreach\", Salary=70000, HoursPerWeek=38),\n",
        "    Row(EmpID=105, Name=\"Divya\", Department=\"Engineering\", Project=\"AI Engine\", Salary=99000, HoursPerWeek=48),\n",
        "    Row(EmpID=106, Name=\"Amit\", Department=\"Marketing\", Project=\"Social Media\", Salary=62000, HoursPerWeek=35),\n",
        "    Row(EmpID=107, Name=\"Priya\", Department=\"HR\", Project=\"Policy Revamp\", Salary=58000, HoursPerWeek=37),\n",
        "    Row(EmpID=108, Name=\"Manav\", Department=\"Sales\", Project=\"Lead Gen\", Salary=73000, HoursPerWeek=41),\n",
        "    Row(EmpID=109, Name=\"Neha\", Department=\"Engineering\", Project=\"Security Suite\", Salary=91000, HoursPerWeek=46),\n",
        "    Row(EmpID=110, Name=\"Farah\", Department=\"HR\", Project=\"Onboarding\", Salary=60000, HoursPerWeek=36)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Create Views\n",
        "\n"
      ],
      "metadata": {
        "id": "7b3i0DwLFlg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Local temp View\n",
        "df.createOrReplaceTempView(\"employees_local\")\n",
        "\n",
        "# Global temp View\n",
        "df.createOrReplaceGlobalTempView(\"employees_global\")\n"
      ],
      "metadata": {
        "id": "IFo-SNn0Fwg8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part A: Exercises on Local View ( employees_local )"
      ],
      "metadata": {
        "id": "aVOWoq1dF5UC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. List all employees working on the \"AI Engine\" project."
      ],
      "metadata": {
        "id": "QtZS-ia2F67X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM employees_local WHERE Project = 'AI Engine' \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8TJCqM5Gh1s",
        "outputId": "88d0c8a5-7f7a-4144-c166-c80f043d76f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------+------+------------+\n",
            "|EmpID| Name| Department|  Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------+------+------------+\n",
            "|  101| Ravi|Engineering|AI Engine| 95000|          42|\n",
            "|  105|Divya|Engineering|AI Engine| 99000|          48|\n",
            "+-----+-----+-----------+---------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Show all employees from the \"Marketing\" department with salaries greater than\n",
        "60,000."
      ],
      "metadata": {
        "id": "Z1zdeEwfF90q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM employees_local where Department = 'Marketing' AND salary> 60000\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSGE8lOOG_py",
        "outputId": "f86b2958-455d-4dd0-8ce3-90e4d77faefa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+--------------+------+------------+\n",
            "|EmpID| Name|Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "|  103|Kabir| Marketing|Product Launch| 65000|          40|\n",
            "|  106| Amit| Marketing|  Social Media| 62000|          35|\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Calculate the average salary for each department."
      ],
      "metadata": {
        "id": "53TKSLE0F_kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\" select Department, Avg(salary) as AvgSalary from employees_local group by Department\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmY1wbgqHs03",
        "outputId": "1a53e4ff-795f-45ab-e686-f0de90218fb6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+\n",
            "| Department|AvgSalary|\n",
            "+-----------+---------+\n",
            "|      Sales|  71500.0|\n",
            "|Engineering|  93000.0|\n",
            "|  Marketing|  63500.0|\n",
            "|         HR|  59000.0|\n",
            "+-----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. List the top 3 highest paid employees overall."
      ],
      "metadata": {
        "id": "CbTIa8mQGBnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from employees_local order by Salary DESC LIMIT 3\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86r8Ihq-IT8f",
        "outputId": "b52c83cf-4b26-4ffc-fec6-012e7d786e4d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Find employees who work more than 40 hours per week.\n"
      ],
      "metadata": {
        "id": "u3vcmJZkGDXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from employees_local where HoursPerWeek > 40\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neF5cD7lIqY7",
        "outputId": "d28acdb6-0932-450c-ffb5-227e4b9379cb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering| Data Platform| 87000|          45|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  108|Manav|      Sales|      Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Group by project and display the number of employees per project."
      ],
      "metadata": {
        "id": "eREj5ohXGGCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select Project, Count(*) as Employees_count from employees_local group by project  \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uREcQJ0HI40Q",
        "outputId": "9deef6ac-02c2-4c74-b240-f82c58261fbe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---------------+\n",
            "|        Project|Employees_count|\n",
            "+---------------+---------------+\n",
            "|  Data Platform|              1|\n",
            "|      AI Engine|              2|\n",
            "| Product Launch|              1|\n",
            "|Client Outreach|              1|\n",
            "| Security Suite|              1|\n",
            "|  Policy Revamp|              1|\n",
            "|       Lead Gen|              1|\n",
            "|   Social Media|              1|\n",
            "|     Onboarding|              1|\n",
            "+---------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Drop the local view. Try querying again — what happens?"
      ],
      "metadata": {
        "id": "Xy09XSg5GHkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.catalog.dropTempView(\"employees_local\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM employees_local\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "lQKyPqR-KgMs",
        "outputId": "85393d38-2e1b-4bee-d12d-6472801a07e1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `employees_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [employees_local], [], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2311506556.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"employees_local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM employees_local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                 )\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `employees_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [employees_local], [], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B: Exercises on Global View ( employees_global )"
      ],
      "metadata": {
        "id": "g9bm-DsDGIvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Retrieve all \"HR\" employees working fewer than 38 hours/week."
      ],
      "metadata": {
        "id": "--sWOo4uGNAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM global_temp.employees_global WHERE Department = 'HR' AND HoursPerWeek < 38\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SHUQsyYUa09",
        "outputId": "6a337725-843e-4285-d00c-154f0b361241"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+-------------+------+------------+\n",
            "|EmpID| Name|Department|      Project|Salary|HoursPerWeek|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "|  107|Priya|        HR|Policy Revamp| 58000|          37|\n",
            "|  110|Farah|        HR|   Onboarding| 60000|          36|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Calculate the total salary payout for each department."
      ],
      "metadata": {
        "id": "10fwedJpGPEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select Department, SUM(Salary) as TotalPayout from global_temp.employees_global group by Department\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL69PZOBUlFA",
        "outputId": "20060685-c7aa-4ed4-ec8b-5a2ddbc462be"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "| Department|TotalPayout|\n",
            "+-----------+-----------+\n",
            "|      Sales|     143000|\n",
            "|Engineering|     372000|\n",
            "|  Marketing|     127000|\n",
            "|         HR|     118000|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. For each employee, add a derived column Status :\n",
        "If HoursPerWeek > 45 → 'Overworked'\n",
        "Otherwise → 'Normal'"
      ],
      "metadata": {
        "id": "I_1pnHalGQnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT *,\n",
        "       CASE WHEN HoursPerWeek > 45 THEN 'Overworked' ELSE 'Normal' END AS Status\n",
        "FROM global_temp.employees_global\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbIsgZ0YU9_S",
        "outputId": "233f8680-a2f9-41ea-ced8-a218dc6d98fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|    Status|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|    Normal|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|    Normal|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|    Normal|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|    Normal|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|Overworked|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|    Normal|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|    Normal|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|    Normal|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|Overworked|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|    Normal|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Count the total number of employees working on each \"Project\" ."
      ],
      "metadata": {
        "id": "KozI8UG9GS88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select Project, Count(*) as Total from global_temp.employees_global group by Project\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuClrekXVq0k",
        "outputId": "3c1348ce-28cc-4bd8-84b1-08a7a63177ba"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----+\n",
            "|        Project|Total|\n",
            "+---------------+-----+\n",
            "|  Data Platform|    1|\n",
            "|      AI Engine|    2|\n",
            "| Product Launch|    1|\n",
            "|Client Outreach|    1|\n",
            "| Security Suite|    1|\n",
            "|  Policy Revamp|    1|\n",
            "|       Lead Gen|    1|\n",
            "|   Social Media|    1|\n",
            "|     Onboarding|    1|\n",
            "+---------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. List employees whose salary is above the average salary in their department."
      ],
      "metadata": {
        "id": "DlVF0CVhGV0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "WITH dept_avg AS (\n",
        "  select Department, AVG(Salary) as avg_salary\n",
        "  from global_temp.employees_global\n",
        "  group by Department\n",
        ")\n",
        "select e.*\n",
        "from global_temp.employees_global e\n",
        "join dept_avg a on e.Department = a.Department\n",
        "where e.Salary> a.avg_salary\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GY3lDEeZNYx",
        "outputId": "d259b384-3ca5-4e00-cb72-72d65765ab3c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  103|Kabir|  Marketing|Product Launch| 65000|          40|\n",
            "|  108|Manav|      Sales|      Lead Gen| 73000|          41|\n",
            "|  110|Farah|         HR|    Onboarding| 60000|          36|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Open a new Spark session and query \"global_temp.employees_global\" from there."
      ],
      "metadata": {
        "id": "ZXYPiKf1GXVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_spark = SparkSession.builder.appName(\"NewSession\").getOrCreate()\n",
        "\n",
        "new_spark.sql(\"SELECT * FROM global_temp.employees_global\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9OfCyxsahRt",
        "outputId": "a4fd5dd0-70bd-47e7-f5d5-7e3cfd403052"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Challenges"
      ],
      "metadata": {
        "id": "l2VLD8bUGYFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank"
      ],
      "metadata": {
        "id": "vWJIzagUa25Q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Use a window function to assign rank to employees within each department based\n",
        "on salary."
      ],
      "metadata": {
        "id": "-6OwNS7mGZ-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "window_spec = Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
        "df.withColumn(\"SalaryRank\", rank().over(window_spec)).select(\"Name\", \"Department\", \"Salary\", \"SalaryRank\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8AI0Ezs5a2aN",
        "outputId": "322e49fb-9579-4cda-a3ae-712d99177ccf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------+------+----------+\n",
            "| Name| Department|Salary|SalaryRank|\n",
            "+-----+-----------+------+----------+\n",
            "|Divya|Engineering| 99000|         1|\n",
            "| Ravi|Engineering| 95000|         2|\n",
            "| Neha|Engineering| 91000|         3|\n",
            "|Sneha|Engineering| 87000|         4|\n",
            "|Farah|         HR| 60000|         1|\n",
            "|Priya|         HR| 58000|         2|\n",
            "|Kabir|  Marketing| 65000|         1|\n",
            "| Amit|  Marketing| 62000|         2|\n",
            "|Manav|      Sales| 73000|         1|\n",
            "|Anita|      Sales| 70000|         2|\n",
            "+-----+-----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Create another view (local or global) that only contains \"Engineering\"\n",
        "employees."
      ],
      "metadata": {
        "id": "K94W8VuXGbwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col(\"Department\") == \"Engineering\").createOrReplaceTempView(\"engineering_employees\")"
      ],
      "metadata": {
        "id": "PqDRXdprcKFj"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create a SQL view that filters out all employees working < 38 hours and saves\n",
        "it as \"active_employees\" ."
      ],
      "metadata": {
        "id": "FhmVoTE8Gd6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col(\"HoursPerWeek\") >= 38).createOrReplaceTempView(\"active_Employees\")\n",
        "spark.sql(\"select * from active_employees\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-UMd4ILccEt",
        "outputId": "631e313f-e844-4c0f-a003-5346589e23db"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}