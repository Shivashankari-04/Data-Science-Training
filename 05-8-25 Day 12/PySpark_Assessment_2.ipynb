{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Module 1: Setup & SparkSession Initialization"
      ],
      "metadata": {
        "id": "8weaoMKlhQ9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cRTTOxUgwJC",
        "outputId": "2a154693-142b-420b-b6ed-958980791b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-05 09:17:30--  https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400395283 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.0-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.0-bin-had 100%[===================>] 381.85M   137KB/s    in 43m 46s \n",
            "\n",
            "2025-08-05 10:01:18 (149 KB/s) - ‘spark-3.5.0-bin-hadoop3.tgz’ saved [400395283/400395283]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download Spark\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Spark\n",
        "!tar -xzf spark-3.5.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "AjasZooShYuL"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BotCampus PySpark Practice\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "cXO32EcThfzb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataframe"
      ],
      "metadata": {
        "id": "9AzcX2jlhxh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"Anjali\", \"Bangalore\", 24),\n",
        "    (\"Ravi\", \"Hyderabad\", 28),\n",
        "    (\"Kavya\", \"Delhi\", 22),\n",
        "    (\"Meena\", \"Chennai\", 25),\n",
        "    (\"Arjun\", \"Mumbai\", 30)\n",
        "]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvJBpWaChyG2",
        "outputId": "952f3fb7-b627-45d9-bc16-18ed955a4489"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Anjali|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "| Arjun|   Mumbai| 30|\n",
            "+------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Show schema, explain data types, and convert to RDD."
      ],
      "metadata": {
        "id": "lhr_xqQAh1K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Schema and datatypes\n",
        "df.printSchema()\n",
        "\n",
        "# conversion to RDD\n",
        "rdd = df.rdd\n",
        "print(\"RDD content:\", rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5oaEsezh3J4",
        "outputId": "bb778fc0-5b42-40e2-ac72-021498a58aae"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- dept: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- bonus: integer (nullable = false)\n",
            " |-- total_ctc: integer (nullable = true)\n",
            "\n",
            "RDD content: [Row(emp_id=1, name='Arjun', dept='IT', salary=75000, bonus=5000, total_ctc=80000), Row(emp_id=2, name='Kavya', dept='HR', salary=62000, bonus=2000, total_ctc=64000), Row(emp_id=3, name='Sneha', dept='Finance', salary=68000, bonus=4000, total_ctc=72000), Row(emp_id=4, name='Ramesh', dept='Sales', salary=58000, bonus=2000, total_ctc=60000)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print .collect() and df.rdd.map() output."
      ],
      "metadata": {
        "id": "63Lhp1TNh3ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.rdd.collect())  # full data\n",
        "print(df.rdd.map(lambda x: (x.name, x.city)).collect())  # mapped example\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHCKodUMh_jg",
        "outputId": "e2886013-ae30-450b-b7bb-1d4118ff5b64"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(name='Anjali', city='Bangalore', age=24), Row(name='Ravi', city='Hyderabad', age=28), Row(name='Kavya', city='Delhi', age=22), Row(name='Meena', city='Chennai', age=25), Row(name='Arjun', city='Mumbai', age=30)]\n",
            "[('Anjali', 'Bangalore'), ('Ravi', 'Hyderabad'), ('Kavya', 'Delhi'), ('Meena', 'Chennai'), ('Arjun', 'Mumbai')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2: RDDs & Transformations"
      ],
      "metadata": {
        "id": "5JUprIjVh_FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = spark.sparkContext.parallelize([\n",
        "    \"Ravi from Bangalore loved the delivery\",\n",
        "    \"Meena from Hyderabad had a late order\",\n",
        "    \"Ajay from Pune liked the service\",\n",
        "    \"Anjali from Delhi faced UI issues\",\n",
        "    \"Rohit from Mumbai gave positive feedback\"\n",
        "])"
      ],
      "metadata": {
        "id": "MhqN268viCRI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1:  Split each line into words (\n",
        " flatMap )."
      ],
      "metadata": {
        "id": "QN-caI38iDEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = feedback.flatMap(lambda x: x.lower().split())"
      ],
      "metadata": {
        "id": "ZHX_fqtXiF_M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Remove stop words (from , the ,etc.)."
      ],
      "metadata": {
        "id": "X3hobgK7iGYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = {\"From\", \"the\", \"a\", \"an\", \"had\"}\n",
        "filtered = words.filter(lambda x: x not in stopwords)"
      ],
      "metadata": {
        "id": "EqjBTuEDiT-W"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Count each word frequency using reduceByKey."
      ],
      "metadata": {
        "id": "0y9ARQUwiUYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_pairs = filtered.map(lambda word: (word,1))\n",
        "word_counts = word_pairs.reduceByKey(lambda a, b: a+b)\n",
        "word_counts.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9SKslVbiqiu",
        "outputId": "4b4056e7-57dc-4a8a-974f-346de17e656f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('from', 5),\n",
              " ('loved', 1),\n",
              " ('liked', 1),\n",
              " ('service', 1),\n",
              " ('anjali', 1),\n",
              " ('faced', 1),\n",
              " ('issues', 1),\n",
              " ('rohit', 1),\n",
              " ('mumbai', 1),\n",
              " ('positive', 1),\n",
              " ('feedback', 1),\n",
              " ('ravi', 1),\n",
              " ('bangalore', 1),\n",
              " ('delivery', 1),\n",
              " ('meena', 1),\n",
              " ('hyderabad', 1),\n",
              " ('late', 1),\n",
              " ('order', 1),\n",
              " ('ajay', 1),\n",
              " ('pune', 1),\n",
              " ('delhi', 1),\n",
              " ('ui', 1),\n",
              " ('gave', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Find top 3 most frequent non- stop words"
      ],
      "metadata": {
        "id": "ZtxBr9_RirBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top3 = word_counts.takeOrdered(3, key=lambda x: -x[-1])\n",
        "print(\"Top 3 frequent words:\", top3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fjs9__Niv_y",
        "outputId": "69851987-90ad-459f-f929-a758de755702"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 frequent words: [('from', 5), ('loved', 1), ('liked', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 3: DataFrames & Transformation (With Joins)\n"
      ],
      "metadata": {
        "id": "iaZiHyu-ix5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "students = [\n",
        "    (\"Amit\", \"10-A\", 89),\n",
        "    (\"Kavya\", \"10-B\", 92),\n",
        "    (\"Anjali\", \"10-A\", 78),\n",
        "    (\"Rohit\", \"10-B\", 85),\n",
        "    (\"Sneha\", \"10-C\", 80)\n",
        "]\n",
        "attendance = [\n",
        "    (\"Amit\", 24),\n",
        "    (\"Kavya\", 22),\n",
        "    (\"Anjali\", 20),\n",
        "    (\"Rohit\", 25),\n",
        "    (\"Sneha\", 19)\n",
        "]\n",
        "columns1= [\"name\", \"section\", \"marks\"]\n",
        "columns2= [\"name\", \"days_present\"]\n",
        "\n",
        "df_students = spark.createDataFrame(students, columns1)\n",
        "df_attendance = spark.createDataFrame(attendance, columns2)"
      ],
      "metadata": {
        "id": "b6jYM4zsi0iY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tak 1: Join both DataFrames on\n",
        "name ."
      ],
      "metadata": {
        "id": "W4JHZdKHi13k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_joined = df_students.join(df_attendance, \"name\")\n",
        "df_joined.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3p7jWKk0i4xD",
        "outputId": "c1246ed8-f3ef-47b7-ade2-791347cb5c6b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+------------+\n",
            "|  name|section|marks|grade|days_present|\n",
            "+------+-------+-----+-----+------------+\n",
            "|  Amit|   10-A|   89|    B|          24|\n",
            "|Anjali|   10-A|   78|    C|          20|\n",
            "| Kavya|   10-B|   92|    A|          22|\n",
            "| Rohit|   10-B|   85|    B|          25|\n",
            "| Sneha|   10-C|   80|    B|          19|\n",
            "+------+-------+-----+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2. Create a new column:\n",
        "attendance_rate = days_present / 25 ."
      ],
      "metadata": {
        "id": "wmbFvJGNi5Cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_joined = df_joined.withColumn(\"attendance_rate\", col(\"days_present\")/25)\n",
        "df_joined.select(\"name\", \"attendance_rate\", \"days_present\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaQSU0Pui8Bz",
        "outputId": "ed6c1c5b-a021-407c-dd7a-0c6c16d2cced"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------------+------------+\n",
            "|  name|attendance_rate|days_present|\n",
            "+------+---------------+------------+\n",
            "|  Amit|           0.96|          24|\n",
            "|Anjali|            0.8|          20|\n",
            "| Kavya|           0.88|          22|\n",
            "| Rohit|            1.0|          25|\n",
            "| Sneha|           0.76|          19|\n",
            "+------+---------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Grade students using\n",
        "when :\n",
        " A: >90, B: 80–90, C: <80."
      ],
      "metadata": {
        "id": "e3ExcLlFi8Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "df_joined = df_joined.withColumn(\"grade\",\n",
        "            when(col(\"marks\") > 90, \"A\")\n",
        "            .when(col(\"marks\") >=80, \"B\")\n",
        "            .otherwise(\"C\"))\n",
        "df_joined.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P1bdhb_i_D7",
        "outputId": "effe1c16-3dd4-4920-8209-bf3369a5543a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+-----+--------------+---------------+\n",
            "|  name|section|marks|days_present|grade|attendace_rate|attendance_rate|\n",
            "+------+-------+-----+------------+-----+--------------+---------------+\n",
            "|  Amit|   10-A|   89|          24|    B|          0.96|           0.96|\n",
            "|Anjali|   10-A|   78|          20|    C|           0.8|            0.8|\n",
            "| Kavya|   10-B|   92|          22|    A|          0.88|           0.88|\n",
            "| Rohit|   10-B|   85|          25|    B|           1.0|            1.0|\n",
            "| Sneha|   10-C|   80|          19|    B|          0.76|           0.76|\n",
            "+------+-------+-----+------------+-----+--------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Filter students with good grades but poor attendance (<80%)."
      ],
      "metadata": {
        "id": "FVqx8bjpi_aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_joined.filter((col(\"grade\").isin(\"A\", \"B\")) &(col(\"attendance_rate\")<0.8)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pOkjz4PjCuX",
        "outputId": "0b7edc5a-a173-4868-88ff-e3aae0fc73ee"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-----+------------+-----+--------------+---------------+\n",
            "| name|section|marks|days_present|grade|attendace_rate|attendance_rate|\n",
            "+-----+-------+-----+------------+-----+--------------+---------------+\n",
            "|Sneha|   10-C|   80|          19|    B|          0.76|           0.76|\n",
            "+-----+-------+-----+------------+-----+--------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 4: Ingest CSV & JSON, Save to Parquet"
      ],
      "metadata": {
        "id": "LB4QCOUajEfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_content = \"\"\"emp_id,name,dept,city,salary\n",
        "101,Anil,IT,Bangalore,80000\n",
        "102,Kiran,HR,Mumbai,65000\n",
        "103,Deepa,Finance,Chennai,72000\"\"\"\n",
        "with open(\"employees.csv\", \"w\") as f:\n",
        "    f.write(csv_content)\n",
        "\n"
      ],
      "metadata": {
        "id": "yY2zawJajFwO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_content = \"\"\"\n",
        "{\n",
        "  \"id\": 201,\n",
        "  \"name\": \"Nandini\",\n",
        "  \"contact\": {\n",
        "    \"email\": \"nandi@example.com\",\n",
        "    \"city\": \"Hyderabad\"\n",
        "  },\n",
        "  \"skills\": [\"Python\", \"Spark\", \"SQL\"]\n",
        "}\n",
        "\"\"\"\n",
        "with open(\"employee.json\", \"w\") as f:\n",
        "    f.write(json_content)\n"
      ],
      "metadata": {
        "id": "mPiMBY55jJSQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Read both formats into DataFrames.\n"
      ],
      "metadata": {
        "id": "qk6_hoGXjHAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
        "df_csv.show()\n",
        "df_json = spark.read.json(\"employee.json\", multiLine=True)\n",
        "df_json.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAprNPmmjNky",
        "outputId": "1b781cab-a725-44bc-e027-fe34836ad37f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+---------+------+\n",
            "|emp_id| name|   dept|     city|salary|\n",
            "+------+-----+-------+---------+------+\n",
            "|   101| Anil|     IT|Bangalore| 80000|\n",
            "|   102|Kiran|     HR|   Mumbai| 65000|\n",
            "|   103|Deepa|Finance|  Chennai| 72000|\n",
            "+------+-----+-------+---------+------+\n",
            "\n",
            "+------------------------------+---+-------+--------------------+\n",
            "|contact                       |id |name   |skills              |\n",
            "+------------------------------+---+-------+--------------------+\n",
            "|{Hyderabad, nandi@example.com}|201|Nandini|[Python, Spark, SQL]|\n",
            "+------------------------------+---+-------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Flatten nested JSON using\n",
        "select ,\n",
        "col ,\n",
        "alias ,\n",
        "explode"
      ],
      "metadata": {
        "id": "eJhvkdf3jN-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "\n",
        "df_json_flat = df_json.select(\n",
        "    \"id\", \"name\",\n",
        "    col(\"contact.email\").alias(\"email\"),\n",
        "    col(\"contact.city\").alias(\"city\"),\n",
        "    explode(\"skills\").alias(\"skill\"))\n",
        "df_json_flat.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSbtzCutjQxn",
        "outputId": "05a1bded-683b-47b9-d383-f6377e6eb1df"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-----------------+---------+------+\n",
            "|id |name   |email            |city     |skill |\n",
            "+---+-------+-----------------+---------+------+\n",
            "|201|Nandini|nandi@example.com|Hyderabad|Python|\n",
            "|201|Nandini|nandi@example.com|Hyderabad|Spark |\n",
            "|201|Nandini|nandi@example.com|Hyderabad|SQL   |\n",
            "+---+-------+-----------------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Save both as Parquet files partitioned by city."
      ],
      "metadata": {
        "id": "tZDVi7KKjRZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"/tmp/employees_csv\")\n",
        "df_json_flat.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"/tmp/employees_json\")\n"
      ],
      "metadata": {
        "id": "Chle2yRpjT4v"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 5: Spark SQL with Temp Views"
      ],
      "metadata": {
        "id": "9321f2u7jTgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_students.createOrReplaceTempView(\"students_view\")"
      ],
      "metadata": {
        "id": "2PU4x-FEjaNV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Task 1: Average marks per section"
      ],
      "metadata": {
        "id": "v-oRx83ujaj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spark.sql(\"select section, AVG(marks) as avg_marks from students_view group by section\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiY3zUhljdI1",
        "outputId": "c194a661-db9c-442f-86c9-a483bf480383"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|section|avg_marks|\n",
            "+-------+---------+\n",
            "|   10-A|     83.5|\n",
            "|   10-B|     88.5|\n",
            "|   10-C|     80.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2. Top scorer in each section"
      ],
      "metadata": {
        "id": "ZOJBNdVNjdim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "      select section, name, marks from(\n",
        "        select *, rank() over(partition by section order by marks desc) as rnk\n",
        "        from students_view)\n",
        "        where rnk = 1\n",
        "        \"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjnacT7hjfwL",
        "outputId": "fce31a0e-c7b3-40d1-ba4a-b7b251a937e8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+\n",
            "|section| name|marks|\n",
            "+-------+-----+-----+\n",
            "|   10-A| Amit|   89|\n",
            "|   10-B|Kavya|   92|\n",
            "|   10-C|Sneha|   80|\n",
            "+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Count of students in each grade category"
      ],
      "metadata": {
        "id": "oFZkaMdQjgAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_students = df_students.withColumn(\"grade\",\n",
        "                when(col(\"marks\") > 90, \"A\")\n",
        "                .when(col(\"marks\") >=80, \"B\")\n",
        "                .otherwise(\"C\"))\n",
        "df_students.createOrReplaceTempView(\"graded_students\")\n",
        "spark.sql(\"select grade, count(*) as count from graded_students group by grade\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT9MPgkKjiMG",
        "outputId": "ad892c94-b292-4a16-e7d0-c9874f91c080"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[grade: string, count: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4. Students with marks above class average"
      ],
      "metadata": {
        "id": "Ba5iWTDrjidV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "  select * from students_view\n",
        "  where marks>(select avg(marks) from students_view)\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RJIQ16rjkbZ",
        "outputId": "d354bab0-0032-496f-bf86-9af6df4e5812"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-----+\n",
            "| name|section|marks|\n",
            "+-----+-------+-----+\n",
            "| Amit|   10-A|   89|\n",
            "|Kavya|   10-B|   92|\n",
            "|Rohit|   10-B|   85|\n",
            "+-----+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5. Attendance-adjusted performance"
      ],
      "metadata": {
        "id": "vxDXJo1Njkr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_combined = df_students.join(df_attendance, \"name\")\n",
        "df_combined = df_combined.withColumn(\"adjusted_grade\",\n",
        "    when(col(\"days_present\") < 20, \"D\").otherwise(col(\"grade\"))\n",
        ")\n",
        "df_combined.select(\"name\", \"section\", \"grade\", \"days_present\", \"adjusted_grade\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr5v-obijnHv",
        "outputId": "62ee6ca1-6351-4ace-80f7-c2f1aa1af916"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+--------------+\n",
            "|  name|section|grade|days_present|adjusted_grade|\n",
            "+------+-------+-----+------------+--------------+\n",
            "|  Amit|   10-A|    B|          24|             B|\n",
            "|Anjali|   10-A|    C|          20|             C|\n",
            "| Kavya|   10-B|    A|          22|             A|\n",
            "| Rohit|   10-B|    B|          25|             B|\n",
            "| Sneha|   10-C|    B|          19|             D|\n",
            "+------+-------+-----+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 6: Partitioned Data & Incremental Loading"
      ],
      "metadata": {
        "id": "DiCZCgPZjpDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Load\n",
        "df_students.write.partitionBy(\"section\").parquet(\"output/students/\")\n"
      ],
      "metadata": {
        "id": "tLuJu-NcjrDA"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Incremental Load\n",
        "incremental = [(\"Tejas\", \"10-A\", 91)]\n",
        "df_inc = spark.createDataFrame(incremental, [\"name\", \"section\", \"marks\"])\n",
        "df_inc.write.mode(\"append\").partitionBy(\"section\").parquet(\"output/students/\")\n"
      ],
      "metadata": {
        "id": "e7G_L1ENjtMj"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: List files in\n",
        "output/students/ using Python."
      ],
      "metadata": {
        "id": "j54G9bSOjwP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Partitions:\", os.listdir(\"output/students/\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iJQ8cwEjywm",
        "outputId": "f328639e-dd3a-4697-a47c-0e1da3962e0a"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions: ['._SUCCESS.crc', 'section=10-A', 'section=10-B', '_SUCCESS', 'section=10-C']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Read only partition\n",
        "10-A and list students."
      ],
      "metadata": {
        "id": "_x6iuF3IjzEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_10a = spark.read.parquet(\"output/students/section=10-A\")\n",
        "df_10a.show()\n",
        "print(\"Total students in 10-A:\", df_10a.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SI6Wk_nj1J5",
        "outputId": "b45784dd-21b6-43f2-b9a8-573ed04661bf"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+\n",
            "|  name|marks|grade|\n",
            "+------+-----+-----+\n",
            "|Anjali|   78|    C|\n",
            "|  Amit|   89|    B|\n",
            "+------+-----+-----+\n",
            "\n",
            "Total students in 10-A: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Compare before/after counts for section\n",
        "10-A ."
      ],
      "metadata": {
        "id": "86jpxDnKj1iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before_count = df_10a.count()\n",
        "print(\"Before incremental load:\", before_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18EY_qrwj5z0",
        "outputId": "b4c2c0ff-1a5b-466a-97df-217e5d75f74c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before incremental load: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# after incrementing the load\n",
        "df_10a_updated = spark.read.parquet(\"output/students/section=10-A\")\n",
        "after_count = df_10a_updated.count()\n",
        "print(\"After incremental load:\", after_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xsWdbksx6QP",
        "outputId": "79b2d79e-e6b3-4e65-8f41-671410e89dea"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After incremental load: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 7: ETL Pipeline – End to End"
      ],
      "metadata": {
        "id": "Af49uWtVj6M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data = \"\"\"emp_id,name,dept,salary,bonus\n",
        "1,Arjun,IT,75000,5000\n",
        "2,Kavya,HR,62000,\n",
        "3,Sneha,Finance,68000,4000\n",
        "4,Ramesh,Sales,58000,\"\"\"\n",
        "with open(\"emp_data.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n"
      ],
      "metadata": {
        "id": "k3wbvzDNj7_k"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1:  Load CSV with inferred schema."
      ],
      "metadata": {
        "id": "5FPl1cCLj8U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"emp_data.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "EgSLwQg0j-xa"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Fill null bonuses with\n",
        "2000 ."
      ],
      "metadata": {
        "id": "te5eB6FDj_Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.fillna({\"bonus\": 2000})"
      ],
      "metadata": {
        "id": "Ec0LEiRIkCfd"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Create\n",
        "total_ctc = salary + bonus ."
      ],
      "metadata": {
        "id": "gNW_PmAPkCvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn(\"total_ctc\", col(\"salary\") + col(\"bonus\"))"
      ],
      "metadata": {
        "id": "bGkuqBjhkFmT"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Filter employees with\n",
        "total_ctc > 65000"
      ],
      "metadata": {
        "id": "zmmedVLZkGKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df.filter(col(\"total_ctc\") > 65000)"
      ],
      "metadata": {
        "id": "M9QVH4u5kFsu"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  Saving result in JSON and\n",
        " Parquet format partitioned by department"
      ],
      "metadata": {
        "id": "qvYGS8SpkIUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.write.mode(\"overwrite\").json(\"/tmp/final_emps_json\")\n",
        "df_final.write.mode(\"overwrite\").partitionBy(\"dept\").parquet(\"/tmp/final_emps_parquet\")"
      ],
      "metadata": {
        "id": "zsKiWzfVkTft"
      },
      "execution_count": 80,
      "outputs": []
    }
  ]
}