{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. PySpark Setup & Initialization"
      ],
      "metadata": {
        "id": "I4QBZpAc7Ug-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1.1 – Setup Spark:"
      ],
      "metadata": {
        "id": "R5ZrwTDJH7He"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANnpptMK5laf",
        "outputId": "562c651a-216d-42b3-8cab-3dc9a2a2b3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-05 06:28:24--  https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 400395283 (382M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.5.0-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.5.0-bin-had 100%[===================>] 381.85M  2.15MB/s    in 6m 41s  \n",
            "\n",
            "2025-08-05 06:35:06 (976 KB/s) - ‘spark-3.5.0-bin-hadoop3.tgz’ saved [400395283/400395283]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download Spark\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BotCampus Intermediate Session\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "mrswLNIB62Wp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1.2 – Load starter data"
      ],
      "metadata": {
        "id": "PrnwRv6b7Tv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " data = [(\"Ananya\", \"Bangalore\", 24),\n",
        "(\"Ravi\", \"Hyderabad\", 28),\n",
        "(\"Kavya\", \"Delhi\", 22),\n",
        "(\"Meena\", \"Chennai\", 25)]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnjXDTwO7S2C",
        "outputId": "515b392b-de91-41a2-92c0-bcd44b3a62f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Ananya|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "+------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. RDDs & Transformations"
      ],
      "metadata": {
        "id": "v5fpnh-W7iXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Exercise 2.1 – Create RDD from feedback"
      ],
      "metadata": {
        "id": "gHpf7DP-7lIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " feedback = spark.sparkContext.parallelize([\n",
        "\"Ravi from Bangalore loved the mobile app\",\n",
        "\"Meena from Delhi reported poor response time\",\n",
        "\"Ajay from Pune liked the delivery speed\",\n",
        "\"Ananya from Hyderabad had an issue with UI\",\n",
        "\"Rohit from Mumbai gave positive feedback\"\n",
        "])"
      ],
      "metadata": {
        "id": "h9EL9iZz7pi9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Count total number of words."
      ],
      "metadata": {
        "id": "jMNEssNB7qTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_words = feedback.flatMap(lambda x: x.split()).count()\n",
        "print(\"Total Words:\", total_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wwAi7RP7tZz",
        "outputId": "6300e8dc-760b-476e-c5f9-43954d6e7572"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Words: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2. Find top 3 most common words."
      ],
      "metadata": {
        "id": "_3DZWfU57u2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "top3 = Counter(feedback.flatMap(lambda x: x.split()).collect()).most_common(3)\n",
        "print(\"Top 3 words:\", top3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhulGBdO7x3X",
        "outputId": "8a5c1898-b8f7-4b3b-d5a9-133bba93d66f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 words: [('from', 5), ('the', 2), ('Ravi', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Remove stop words (from , with , the etc.)."
      ],
      "metadata": {
        "id": "mSabSkrC7yRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = {\"from\", \"with\", \"the\", \"an\", \"and\", \"had\"}\n",
        "filtered_words = feedback.flatMap(lambda x: [w for w in x.lower().split() if w not in stopwords])\n",
        "print(\"Filtered words:\", filtered_words.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHEbPmG378Gd",
        "outputId": "8e490752-1781-4b61-b9c8-8f6a3050e83e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered words: ['ravi', 'bangalore', 'loved', 'mobile', 'app', 'meena', 'delhi', 'reported', 'poor', 'response', 'time', 'ajay', 'pune', 'liked', 'delivery', 'speed', 'ananya', 'hyderabad', 'issue', 'ui', 'rohit', 'mumbai', 'gave', 'positive', 'feedback']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Create a dictionary of word → count."
      ],
      "metadata": {
        "id": "6JHMQRpy78kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_count_dict = dict(Counter(filtered_words.collect()))\n",
        "print(\"Word counts:\", word_count_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKLhHwF_7-UK",
        "outputId": "0e77f01b-233a-4295-d4ff-6bed9ed2c351"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word counts: {'ravi': 1, 'bangalore': 1, 'loved': 1, 'mobile': 1, 'app': 1, 'meena': 1, 'delhi': 1, 'reported': 1, 'poor': 1, 'response': 1, 'time': 1, 'ajay': 1, 'pune': 1, 'liked': 1, 'delivery': 1, 'speed': 1, 'ananya': 1, 'hyderabad': 1, 'issue': 1, 'ui': 1, 'rohit': 1, 'mumbai': 1, 'gave': 1, 'positive': 1, 'feedback': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. DataFrames – Transformations"
      ],
      "metadata": {
        "id": "1LsLxkm27-s6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Exercise 3.1 – Create exam_scores DataFrame:"
      ],
      "metadata": {
        "id": "2d8A4Erd8C0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col, upper\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank"
      ],
      "metadata": {
        "id": "E3ztt_yw-4Xy"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = [\n",
        "    (\"Ravi\", \"Math\", 88),\n",
        "    (\"Ananya\", \"Science\", 92),\n",
        "    (\"Kavya\", \"English\", 79),\n",
        "    (\"Ravi\", \"English\", 67),\n",
        "    (\"Neha\", \"Math\", 94),\n",
        "    (\"Meena\", \"Science\", 85)\n",
        "]\n",
        "columns = [\"name\", \"subject\", \"score\"]\n",
        "df_scores = spark.createDataFrame(scores, columns)"
      ],
      "metadata": {
        "id": "IQWtaqtU8J_p"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Task 1: Add grade column (>=90 → A, 80-89 → B,\n",
        "70-79 → C, else D)"
      ],
      "metadata": {
        "id": "Z7HtdGYp8Kas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores = df_scores.withColumn(\"grade\", when(col(\"score\") >= 90, \"A\")\n",
        "                                 .when(col(\"score\") >= 80, \"B\")\n",
        "                                 .when(col(\"score\") >= 70, \"C\")\n",
        "                                 .otherwise(\"D\"))\n",
        "df_scores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyUIhmly8Re7",
        "outputId": "7d5462a4-4f98-4005-e97f-ed447284fedf"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+\n",
            "|  name|subject|score|grade|\n",
            "+------+-------+-----+-----+\n",
            "|  Ravi|   Math|   88|    B|\n",
            "|Ananya|Science|   92|    A|\n",
            "| Kavya|English|   79|    C|\n",
            "|  Ravi|English|   67|    D|\n",
            "|  Neha|   Math|   94|    A|\n",
            "| Meena|Science|   85|    B|\n",
            "+------+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Group by subject, find average score."
      ],
      "metadata": {
        "id": "tzSUO9Hh8R2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores.groupBy(\"subject\").avg(\"score\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4uOukjq8TeB",
        "outputId": "49819b3f-20ea-41fd-da21-64e3fc823681"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|subject|avg(score)|\n",
            "+-------+----------+\n",
            "|Science|      88.5|\n",
            "|   Math|      91.0|\n",
            "|English|      73.0|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3. Use when and otherwise to classify subject difficulty (Difficult)."
      ],
      "metadata": {
        "id": "WEQ88qPu8T2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores = df_scores.withColumn(\"difficulty\",\n",
        "            when(col(\"subject\").isin(\"Math\", \"Science\"), \"Difficult\")\n",
        "            .otherwise(\"Easy\"))\n",
        "df_scores.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_NcFBp-8X5S",
        "outputId": "7f11a8e5-374e-48fc-f0a6-0bdf9648223a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+\n",
            "|  name|subject|score|grade|difficulty|\n",
            "+------+-------+-----+-----+----------+\n",
            "|  Ravi|   Math|   88|    B| Difficult|\n",
            "|Ananya|Science|   92|    A| Difficult|\n",
            "| Kavya|English|   79|    C|      Easy|\n",
            "|  Ravi|English|   67|    D|      Easy|\n",
            "|  Neha|   Math|   94|    A| Difficult|\n",
            "| Meena|Science|   85|    B| Difficult|\n",
            "+------+-------+-----+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Rank students per subject using Window function."
      ],
      "metadata": {
        "id": "KycIDYz_8YNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "windowSpec = Window.partitionBy(\"subject\").orderBy(col(\"score\").desc())\n",
        "df_scores = df_scores.withColumn(\"rank\", rank().over(windowSpec))\n",
        "df_scores.select(\"name\", \"subject\", \"score\", \"rank\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJy0PU7p8a90",
        "outputId": "e08c0aa5-5b41-4ddb-8801-c5244b51228d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+----+\n",
            "|  name|subject|score|rank|\n",
            "+------+-------+-----+----+\n",
            "| Kavya|English|   79|   1|\n",
            "|  Ravi|English|   67|   2|\n",
            "|  Neha|   Math|   94|   1|\n",
            "|  Ravi|   Math|   88|   2|\n",
            "|Ananya|Science|   92|   1|\n",
            "| Meena|Science|   85|   2|\n",
            "+------+-------+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Apply UDF to format names (e.g., make all uppercase)."
      ],
      "metadata": {
        "id": "AxJAnswE8bUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores = df_scores.withColumn(\"name_upper\", upper(col(\"name\")))\n",
        "df_scores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ums9Ui0Z8fMi",
        "outputId": "4a5c5950-7096-4c53-ac9e-72838cd8697a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+----+----------+\n",
            "|  name|subject|score|grade|difficulty|rank|name_upper|\n",
            "+------+-------+-----+-----+----------+----+----------+\n",
            "| Kavya|English|   79|    C|      Easy|   1|     KAVYA|\n",
            "|  Ravi|English|   67|    D|      Easy|   2|      RAVI|\n",
            "|  Neha|   Math|   94|    A| Difficult|   1|      NEHA|\n",
            "|  Ravi|   Math|   88|    B| Difficult|   2|      RAVI|\n",
            "|Ananya|Science|   92|    A| Difficult|   1|    ANANYA|\n",
            "| Meena|Science|   85|    B| Difficult|   2|     MEENA|\n",
            "+------+-------+-----+-----+----------+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Ingest CSV & JSON – Save to Parquet"
      ],
      "metadata": {
        "id": "9e66rSpl8fkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Dataset 1: CSV file: students.csv"
      ],
      "metadata": {
        "id": "UFXrrro98g-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data = \"\"\"id,name,department,city,salary\n",
        "1,Amit,IT,Bangalore,78000\n",
        "2,Kavya,HR,Chennai,62000\n",
        "3,Arjun,Finance,Hyderabad,55000\"\"\"\n",
        "with open(\"students.csv\", \"w\") as f:\n",
        "    f.write(csv_data)\n"
      ],
      "metadata": {
        "id": "lr3N_GpK8jx-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Dataset 2: JSON file employee_nested.json"
      ],
      "metadata": {
        "id": "LKGH6eQn8kV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_data = \"\"\"\n",
        "[\n",
        "  {\n",
        "    \"id\": 101,\n",
        "    \"name\": \"Sneha\",\n",
        "    \"address\": {\n",
        "      \"city\": \"Mumbai\",\n",
        "      \"pincode\": 400001\n",
        "    },\n",
        "    \"skills\": [\"Python\", \"Spark\"]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "with open(\"employee_nested.json\", \"w\") as f:\n",
        "    f.write(json_data)\n"
      ],
      "metadata": {
        "id": "Wk4FP9eX8qSu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Task 1: Load both datasets into PySpark and Print schema and infer nested structure."
      ],
      "metadata": {
        "id": "dzw4rOmj8zFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv = spark.read.csv(\"students.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show contents and schema\n",
        "df_csv.show()\n",
        "df_csv.printSchema()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmIQzeIr81Mj",
        "outputId": "cd291362-2db2-42d9-8be0-11f82c65d076"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+---------+------+\n",
            "| id| name|department|     city|salary|\n",
            "+---+-----+----------+---------+------+\n",
            "|  1| Amit|        IT|Bangalore| 78000|\n",
            "|  2|Kavya|        HR|  Chennai| 62000|\n",
            "|  3|Arjun|   Finance|Hyderabad| 55000|\n",
            "+---+-----+----------+---------+------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_json = spark.read.json(\"employee_nested.json\", multiLine=True)\n",
        "\n",
        "# Show contents and schema (nested)\n",
        "df_json.show(truncate=False)\n",
        "df_json.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptCcMcq583AT",
        "outputId": "b14abfd1-ed36-4ad1-bf31-ea8a45baa7f4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+---+-----+---------------+\n",
            "|address         |id |name |skills         |\n",
            "+----------------+---+-----+---------------+\n",
            "|{Mumbai, 400001}|101|Sneha|[Python, Spark]|\n",
            "+----------------+---+-----+---------------+\n",
            "\n",
            "root\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- pincode: long (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2. Flatten the JSON (use explode ,select, alias )."
      ],
      "metadata": {
        "id": "Io871Lq583UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "df_flat = df_json.select(\n",
        "    \"id\",\n",
        "    \"name\",\n",
        "    col(\"address.city\").alias(\"city\"),\n",
        "    col(\"address.pincode\").alias(\"pincode\"),\n",
        "    explode(\"skills\").alias(\"skill\")\n",
        ")\n",
        "\n",
        "df_flat.show(truncate=False)\n",
        "df_flat.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpM97DnN88IY",
        "outputId": "59cb5ee8-7232-483d-aac7-de7956d92538"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+-------+------+\n",
            "|id |name |city  |pincode|skill |\n",
            "+---+-----+------+-------+------+\n",
            "|101|Sneha|Mumbai|400001 |Python|\n",
            "|101|Sneha|Mumbai|400001 |Spark |\n",
            "+---+-----+------+-------+------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- pincode: long (nullable = true)\n",
            " |-- skill: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Convert both to Parquet and write to\n",
        "/tmp/output ."
      ],
      "metadata": {
        "id": "NIUOU-RW89ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save CSV DataFrame to Parquet\n",
        "df_csv.write.mode(\"overwrite\").parquet(\"/tmp/output/students_parquet\")\n",
        "\n",
        "#Save flattened JSON DataFrame to Parquet\n",
        "df_flat.write.mode(\"overwrite\").parquet(\"/tmp/output/employees_parquet\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sJfkQrPd8_rK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Spark SQL – Temp Views & Queries"
      ],
      "metadata": {
        "id": "lMrllMTT9AWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 5.1 Create view from exam scores and run:"
      ],
      "metadata": {
        "id": "6D64Pmho9DKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores.createOrReplaceTempView(\"scores\")"
      ],
      "metadata": {
        "id": "0UY3LUAp9GKy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Top scorer per subject"
      ],
      "metadata": {
        "id": "ME3fyC5oDQIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT subject, name, score FROM (\n",
        "  SELECT *, RANK() OVER(PARTITION BY subject ORDER BY score DESC) AS rnk FROM scores\n",
        ") WHERE rnk = 1\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZY5IBChDWor",
        "outputId": "fd1ada5e-fa63-4d9f-9b04-ef6299f81e2f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----+\n",
            "|subject|  name|score|\n",
            "+-------+------+-----+\n",
            "|English| Kavya|   79|\n",
            "|   Math|  Neha|   94|\n",
            "|Science|Ananya|   92|\n",
            "+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Count of students per grade"
      ],
      "metadata": {
        "id": "lkTQwgswDbT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT grade, COUNT(*) as count FROM scores GROUP BY grade\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7lTuWfFDcNN",
        "outputId": "f59af095-b984-4cd3-8fe8-9c56643047de"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|grade|count|\n",
            "+-----+-----+\n",
            "|    B|    2|\n",
            "|    C|    1|\n",
            "|    A|    2|\n",
            "|    D|    1|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Students with multiple subjects"
      ],
      "metadata": {
        "id": "tmFl_YhoDhkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT name FROM scores\n",
        "GROUP BY name HAVING COUNT(DISTINCT subject) > 1\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcnlamouDiN5",
        "outputId": "2f926ea1-12ea-4f4d-d8de-bbe62a71f2b9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+\n",
            "|name|\n",
            "+----+\n",
            "|Ravi|\n",
            "+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Subjects with avg score > 85"
      ],
      "metadata": {
        "id": "uvRfMyXWDmJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"\n",
        "SELECT subject, AVG(score) as avg_score\n",
        "FROM scores\n",
        "GROUP BY subject\n",
        "HAVING avg_score > 85\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KDCu17dDmyu",
        "outputId": "d831dcfe-c075-45b5-a4c9-6ccefa0ba129"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|subject|avg_score|\n",
            "+-------+---------+\n",
            "|Science|     88.5|\n",
            "|   Math|     91.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Exercise 5.2 Create another DataFrame\n",
        "attendance(name, days_present) and"
      ],
      "metadata": {
        "id": "K55Wjrki9GkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attendance_data = [\n",
        "    (\"Ravi\", 18),\n",
        "    (\"Ananya\", 25),\n",
        "    (\"Kavya\", 22),\n",
        "    (\"Neha\", 30),\n",
        "    (\"Meena\", 15)\n",
        "]\n",
        "\n",
        "att = spark.createDataFrame(attendance_data, [\"name\", \"days_present\"])\n",
        "att.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhJUIPWcEMqk",
        "outputId": "7b36948a-f9af-4b37-cfdc-6f20f1244db1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------+\n",
            "|  name|days_present|\n",
            "+------+------------+\n",
            "|  Ravi|          18|\n",
            "|Ananya|          25|\n",
            "| Kavya|          22|\n",
            "|  Neha|          30|\n",
            "| Meena|          15|\n",
            "+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Join with scores"
      ],
      "metadata": {
        "id": "1r-QPMtZERuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = df_scores.join(att, on=\"name\")\n",
        "joined_df.select(\"name\", \"subject\", \"score\", \"grade\", \"days_present\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npPJIEstEUNb",
        "outputId": "03b3553c-7aa7-41f6-a043-31547f49ab34"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+------------+\n",
            "|  name|subject|score|grade|days_present|\n",
            "+------+-------+-----+-----+------------+\n",
            "|Ananya|Science|   92|    A|          25|\n",
            "| Kavya|English|   79|    C|          22|\n",
            "| Meena|Science|   85|    B|          15|\n",
            "|  Neha|   Math|   94|    A|          30|\n",
            "|  Ravi|   Math|   88|    B|          18|\n",
            "|  Ravi|English|   67|    D|          18|\n",
            "+------+-------+-----+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Calculate attendance-adjusted grade"
      ],
      "metadata": {
        "id": "PylJdZ5fEbiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "adjusted_df = joined_df.withColumn(\"adj_grade\",\n",
        "    when(col(\"days_present\") < 20,\n",
        "         when(col(\"grade\") == \"A\", \"B\")\n",
        "        .when(col(\"grade\") == \"B\", \"C\")\n",
        "        .when(col(\"grade\") == \"C\", \"D\")\n",
        "        .otherwise(\"D\"))\n",
        "    .otherwise(col(\"grade\"))\n",
        ")\n",
        "\n",
        "adjusted_df.select(\"name\", \"subject\", \"score\", \"grade\", \"days_present\", \"adj_grade\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0znF4AdjEcV9",
        "outputId": "42654a9f-19d7-455c-e441-20bee779660b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+------------+---------+\n",
            "|  name|subject|score|grade|days_present|adj_grade|\n",
            "+------+-------+-----+-----+------------+---------+\n",
            "|Ananya|Science|   92|    A|          25|        A|\n",
            "| Kavya|English|   79|    C|          22|        C|\n",
            "| Meena|Science|   85|    B|          15|        C|\n",
            "|  Neha|   Math|   94|    A|          30|        A|\n",
            "|  Ravi|   Math|   88|    B|          18|        C|\n",
            "|  Ravi|English|   67|    D|          18|        D|\n",
            "+------+-------+-----+-----+------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Partitioned Load"
      ],
      "metadata": {
        "id": "hsNUQM54E4BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial Load\n",
        "df_scores.write.partitionBy(\"subject\").parquet(\"/tmp/scores/\")"
      ],
      "metadata": {
        "id": "BA1-3aoFE-Ny"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Incremental Load\n",
        "incremental = [(\"Meena\", \"Math\", 93)]\n",
        "df_inc = spark.createDataFrame(incremental, columns)\n",
        "df_inc.write.mode(\"append\").partitionBy(\"subject\").parquet(\"/tmp/scores/\")"
      ],
      "metadata": {
        "id": "t0fDsiEmFDly"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: List all folders inside /tmp/scores/"
      ],
      "metadata": {
        "id": "hTfcXpk4FSDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = \"/tmp/scores/\"\n",
        "\n",
        "# List only directories (partitions)\n",
        "partitions = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
        "print(\"Partitions in /tmp/scores/:\")\n",
        "for folder in partitions:\n",
        "    print(folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyafKtioFWyx",
        "outputId": "db5f1640-4186-495c-9fde-51da755ab31e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Partitions in /tmp/scores/:\n",
            "subject=Science\n",
            "subject=English\n",
            "subject=Math\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Read only Math partition and display all entries."
      ],
      "metadata": {
        "id": "Oc2xX3deFbpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_math = spark.read.parquet(\"/tmp/scores/subject=Math\")\n",
        "df_math.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0igDrOkFFw91",
        "outputId": "2886e3a5-148d-4785-e2c2-6fde5e8e5767"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+----------+----+----------+\n",
            "| name|score|grade|difficulty|rank|name_upper|\n",
            "+-----+-----+-----+----------+----+----------+\n",
            "| Neha|   94|    A| Difficult|   1|      NEHA|\n",
            "| Ravi|   88|    B| Difficult|   2|      RAVI|\n",
            "|Meena|   93| NULL|      NULL|NULL|      NULL|\n",
            "+-----+-----+-----+----------+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. ETL: Clean, Transform, Load"
      ],
      "metadata": {
        "id": "cost-qcPF1Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = \"\"\"emp_id,name,dept,salary,bonus\n",
        "1,Arjun,IT,78000,5000\n",
        "2,Kavya,HR,62000,\n",
        "3,Sneha,Finance,55000,3000\"\"\"\n",
        "with open(\"emp_raw.csv\", \"w\") as f:\n",
        "    f.write(raw_data)\n"
      ],
      "metadata": {
        "id": "i8F6q6umF3xP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Task 1: Load data with header.\n"
      ],
      "metadata": {
        "id": "KADlUQshGQRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_etl = spark.read.csv(\"emp_raw.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Show data and schema to verify\n",
        "df_etl.show()\n",
        "df_etl.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8NbesNqGRrc",
        "outputId": "702a48c8-807a-4547-8471-fc226653f5b9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+\n",
            "|emp_id| name|   dept|salary|bonus|\n",
            "+------+-----+-------+------+-----+\n",
            "|     1|Arjun|     IT| 78000| 5000|\n",
            "|     2|Kavya|     HR| 62000| NULL|\n",
            "|     3|Sneha|Finance| 55000| 3000|\n",
            "+------+-----+-------+------+-----+\n",
            "\n",
            "root\n",
            " |-- emp_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- dept: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            " |-- bonus: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Fill missing bonus with 2000."
      ],
      "metadata": {
        "id": "Y3KVpyenGSFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_etl = df_etl.fillna({\"bonus\": 2000})\n",
        "df_etl.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6QVIxIGU_g",
        "outputId": "3ddc98c7-ff20-4534-c0f2-bf10ac47f9d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+\n",
            "|emp_id| name|   dept|salary|bonus|\n",
            "+------+-----+-------+------+-----+\n",
            "|     1|Arjun|     IT| 78000| 5000|\n",
            "|     2|Kavya|     HR| 62000| 2000|\n",
            "|     3|Sneha|Finance| 55000| 3000|\n",
            "+------+-----+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Calculate total_ctc = salary + bonus"
      ],
      "metadata": {
        "id": "VUnqYsCCJfJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_etl = df_etl.withColumn(\"total_ctc\", col(\"salary\") + col(\"bonus\"))\n",
        "df_etl.select(\"emp_id\", \"name\", \"salary\", \"bonus\", \"total_ctc\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVZJLfpdGv0r",
        "outputId": "8f38a9b9-b00e-43be-88be-6e4eab34829a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+------+-----+---------+\n",
            "|emp_id| name|salary|bonus|total_ctc|\n",
            "+------+-----+------+-----+---------+\n",
            "|     1|Arjun| 78000| 5000|    83000|\n",
            "|     2|Kavya| 62000| 2000|    64000|\n",
            "|     3|Sneha| 55000| 3000|    58000|\n",
            "+------+-----+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4. Filter where total_ctc > 60,000"
      ],
      "metadata": {
        "id": "sIv7Y8_dHKQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_etl.filter(col(\"total_ctc\") > 60000)\n",
        "df_final.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8wHeV4zHP5v",
        "outputId": "c5c0294c-c229-43e9-8c92-74e49cea5db0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+----+------+-----+---------+\n",
            "|emp_id| name|dept|salary|bonus|total_ctc|\n",
            "+------+-----+----+------+-----+---------+\n",
            "|     1|Arjun|  IT| 78000| 5000|    83000|\n",
            "|     2|Kavya|  HR| 62000| 2000|    64000|\n",
            "+------+-----+----+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Save final DataFrame to Parquet and JSON"
      ],
      "metadata": {
        "id": "UfNvfTKVHRhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.write.mode(\"overwrite\").parquet(\"/tmp/final_emps\")\n",
        "df_final.write.mode(\"overwrite\").json(\"/tmp/final_emps_json\")"
      ],
      "metadata": {
        "id": "wsUXI3XmHTE0"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}